{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)\n",
    "\n",
    "\n",
    "## Work\n",
    "This code was designed, programmed and tested by\n",
    "* Lorenzo Bonannella \n",
    "* Giacomo Fantino\n",
    "* Farisan Fekri\n",
    "* Giacomo Cauda\n",
    "\n",
    "### Comments on our work\n",
    "In this laboratory we designed a RL approach to play tic-tac-toe. In particular, our work was focused on:\n",
    "1. Creating an action-value function\n",
    "2. Designing a hybrid Montecarlo approach: when a certain threshold `MAX_NUM_ACTIONS=3` is reached, a full-game simultation is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X | X | O |\n",
      " O | X | X |\n",
      "   | O |   |\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple\n",
    "\n",
    "#if the player has selected some squares such that with 3 squares the sum is 15 he won\n",
    "#we have to define the squares in tic tac toe such that this function make sense\n",
    "def win(squares):\n",
    "    return any(sum(c) == 15 for c in combinations(squares, 3))\n",
    "\n",
    "Position = namedtuple('Position', ['x', 'o'])\n",
    "\n",
    "#this state value can become the evaluation function\n",
    "def state_value(position : Position):\n",
    "    if win(position.x):\n",
    "        return 1\n",
    "    elif win(position.o):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8] #magic board that is used for check\n",
    "def print_board(pos):\n",
    "    for x in range(3):\n",
    "        for y in range(3):\n",
    "            idx = 3*x + y\n",
    "            if MAGIC[idx] in pos.x:\n",
    "                print(' X |', end='')\n",
    "            elif MAGIC[idx] in pos.o:\n",
    "                print(' O |', end='')\n",
    "            else:\n",
    "                print('   |', end='')\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "state = Position(x={1, 2, 5, 7}, o={3, 6, 9})\n",
    "print_board(state)\n",
    "state_value(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent1: Action-Value Function\n",
    "\n",
    "The first step was to change the agent so that the computed dictionary is now an action value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((frozenset(), frozenset(), 5), 0.47715282503048256),\n",
       " ((frozenset(), frozenset(), 2), 0.38014217911973874),\n",
       " ((frozenset(), frozenset(), 4), 0.2614545388480996),\n",
       " ((frozenset(), frozenset(), 8), 0.25332569107542413),\n",
       " ((frozenset(), frozenset(), 6), 0.2520724547838263),\n",
       " ((frozenset(), frozenset(), 3), 0.1977963254096396),\n",
       " ((frozenset(), frozenset(), 7), 0.15220787746652853),\n",
       " ((frozenset(), frozenset(), 9), 0.13993995679074497),\n",
       " ((frozenset({4}), frozenset({3}), 2), 0.12874122890512435),\n",
       " ((frozenset({1}), frozenset({8}), 5), 0.1215511972310217),\n",
       " ((frozenset(), frozenset(), 1), 0.12061554957746343),\n",
       " ((frozenset({1, 2, 4, 5}), frozenset({3, 6, 7, 8}), 9), 0.11361512828387071),\n",
       " ((frozenset({1, 2, 3, 5}), frozenset({4, 6, 7, 8}), 9), 0.11361512828387071),\n",
       " ((frozenset({2}), frozenset({3}), 4), 0.11325967004354746),\n",
       " ((frozenset({1, 2, 4, 7}), frozenset({3, 5, 8, 9}), 6), 0.10466174574128355),\n",
       " ((frozenset({5}), frozenset({3}), 2), 0.10466174574128355),\n",
       " ((frozenset({1, 3, 7, 9}), frozenset({2, 4, 6, 8}), 5), 0.10466174574128355),\n",
       " ((frozenset({1, 4, 6, 7}), frozenset({2, 3, 8, 9}), 5), 0.10466174574128355),\n",
       " ((frozenset({4, 6, 7, 8}), frozenset({2, 3, 5, 9}), 1), 0.10466174574128355),\n",
       " ((frozenset({4}), frozenset({7}), 2), 0.102478977001032),\n",
       " ((frozenset({3, 5, 6, 8}), frozenset({1, 2, 7, 9}), 4), 0.09561792499119551),\n",
       " ((frozenset({1, 2, 3, 6}), frozenset({4, 5, 7, 9}), 8), 0.09561792499119551),\n",
       " ((frozenset({1, 2, 3, 9}), frozenset({4, 6, 7, 8}), 5), 0.09561792499119551),\n",
       " ((frozenset({2, 7, 8, 9}), frozenset({1, 3, 4, 6}), 5), 0.09561792499119551),\n",
       " ((frozenset({7}), frozenset({1}), 9), 0.09561792499119551),\n",
       " ((frozenset({2, 6, 8, 9}), frozenset({1, 3, 4, 5}), 7), 0.09561792499119551),\n",
       " ((frozenset({5}), frozenset({7}), 8), 0.09486074574128356),\n",
       " ((frozenset({4}), frozenset({3}), 9), 0.09478552529585071),\n",
       " ((frozenset({4}), frozenset({3}), 5), 0.09420816818387072),\n",
       " ((frozenset({8}), frozenset({6}), 7), 0.09414674473284562)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#game with two random players\n",
    "from copy import deepcopy\n",
    "from random import choice\n",
    "\n",
    "\n",
    "def random_game():\n",
    "    state = Position(set(), set())\n",
    "    available = set(range(1, 10))\n",
    "    trajectory = []\n",
    "\n",
    "    while True:\n",
    "        #first player\n",
    "        x = choice(list(available))\n",
    "        trajectory.append((deepcopy(state), x)) #current state + chosen action\n",
    "        state.x.add(x)\n",
    "        available.remove(x)\n",
    "        \n",
    "        if win(state.x):\n",
    "            break\n",
    "        elif len(available) == 0:\n",
    "            break\n",
    "        \n",
    "        #opponent player (doesn't consider the state)\n",
    "        y = choice(list(available))\n",
    "        state.o.add(y)\n",
    "        available.remove(y)\n",
    "\n",
    "        if win(state.o):\n",
    "            break\n",
    "        elif len(available) == 0:\n",
    "            break\n",
    "    \n",
    "    return trajectory, state #sequence of move for computing the state value function\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "Q_func = defaultdict(float)\n",
    "epsilon = 0.01\n",
    "\n",
    "for step in range(4_000):\n",
    "    trajectory, last_state = random_game()\n",
    "    final_reward = state_value(last_state)\n",
    "    \n",
    "    for (state, action) in trajectory:\n",
    "        #hash_state = (frozenset(state.x), frozenset(state.o))\n",
    "        hash_state = (frozenset(state.x), frozenset(state.o), action)\n",
    "        difference = (final_reward - Q_func[hash_state])\n",
    "        Q_func[hash_state] = Q_func[hash_state] + epsilon*difference\n",
    "\n",
    "sorted(Q_func.items(), key=lambda e : e[1], reverse = True)[:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action 1 reward 0.12061554957746343\n",
      "action 2 reward 0.38014217911973874\n",
      "action 3 reward 0.1977963254096396\n",
      "action 4 reward 0.2614545388480996\n",
      "action 5 reward 0.47715282503048256\n",
      "action 6 reward 0.2520724547838263\n",
      "action 7 reward 0.15220787746652853\n",
      "action 8 reward 0.25332569107542413\n",
      "action 9 reward 0.13993995679074497\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 10):\n",
    "    hash_state = (frozenset({}), frozenset({}), i)\n",
    "    reward = Q_func[hash_state]\n",
    "    print(f\"action {i} reward {reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(Q_dict, state, available):\n",
    "    current_reward = -1000\n",
    "    action = -1\n",
    "    for i in available:\n",
    "        hash_state = (frozenset(state.x), frozenset(state.o), i)\n",
    "        reward = Q_dict[hash_state]\n",
    "        if reward > current_reward:\n",
    "            current_reward = reward\n",
    "            action = i\n",
    "    return action\n",
    "\n",
    "wins = 0\n",
    "draws = 0\n",
    "looses = 0\n",
    "\n",
    "for _ in range(100_000):\n",
    "    state = Position(set(), set())\n",
    "    available = set(range(1, 10))\n",
    "\n",
    "    while True:\n",
    "        #first player\n",
    "        x = agent(Q_func, state, available)\n",
    "        state.x.add(x)\n",
    "        available.remove(x)\n",
    "        \n",
    "        if win(state.x):\n",
    "            wins += 1\n",
    "            break\n",
    "        elif len(available) == 0:\n",
    "            draws += 1\n",
    "            break\n",
    "        \n",
    "        #opponent player (doesn't consider the state)\n",
    "        y = choice(list(available))\n",
    "\n",
    "        state.o.add(y)\n",
    "        available.remove(y)\n",
    "\n",
    "        if win(state.o):\n",
    "            looses += 1\n",
    "            break\n",
    "        elif len(available) == 0:\n",
    "            draws += 1\n",
    "            break\n",
    "print(f'Wins {wins/(wins + looses + draws)}')\n",
    "print(f'Draws {draws/(wins + looses + draws)}')\n",
    "print(f'Looses {looses/(wins + looses + draws)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent2: Hybrid Montecarlo simulation with full game simulation\n",
    "\n",
    "As second agent we have created an hybrid of MonteCarlo and Full game: we start by exploring a certain path but after a certain level of depth we switch to exploitation and explore the entire subtree. The reward is the average of all possible terminal states and that's the value that will be propagated.\n",
    "For nodes below the level we use a pessimistic strategy, assigning the worst reward to the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "Q_func = defaultdict(float)\n",
    "epsilon = 0.01\n",
    "\n",
    "#game with two random players using a hybr\n",
    "def full_game(state, available):\n",
    "    turn_player = 'x' if len(state.x) == len(state.o) else 'o'\n",
    "\n",
    "    if win(state.x):\n",
    "        return [1]\n",
    "    elif win(state.o):\n",
    "        return [-1]\n",
    "    elif len(available) == 0:\n",
    "        return [0]\n",
    "\n",
    "    general_list = []\n",
    "    for act in available:\n",
    "        new_available = deepcopy(available)\n",
    "        new_state = deepcopy(state)\n",
    "        if turn_player == 'x':\n",
    "            new_state.x.add(act)\n",
    "        else:\n",
    "            new_state.o.add(act)\n",
    "        new_available.remove(act)\n",
    "        list_act = full_game(new_state, new_available)\n",
    "\n",
    "        if turn_player == 'x':\n",
    "            #we use min to consider the worst case so the agent will choose more safe actions\n",
    "            hash_state = (frozenset(state.x), frozenset(state.o), action)\n",
    "            Q_func[hash_state] = min(list_act)\n",
    "        general_list.extend(list_act)\n",
    "\n",
    "    return general_list\n",
    "\n",
    "MAX_NUM_ACTIONS = 3\n",
    "def random_game():\n",
    "    state = Position(set(), set())\n",
    "    available = set(range(1, 10))\n",
    "    trajectory = []\n",
    "    num_actions = 0\n",
    "\n",
    "    while num_actions < MAX_NUM_ACTIONS:\n",
    "        #first player\n",
    "        x = choice(list(available))\n",
    "        trajectory.append((deepcopy(state), x)) #current state + chosen action\n",
    "        state.x.add(x)\n",
    "        available.remove(x)\n",
    "        \n",
    "        if win(state.x):\n",
    "            break\n",
    "        elif len(available) == 0:\n",
    "            break\n",
    "        \n",
    "        #opponent player (doesn't consider the state)\n",
    "        y = choice(list(available))\n",
    "        state.o.add(y)\n",
    "        available.remove(y)\n",
    "\n",
    "        if win(state.o):\n",
    "            break\n",
    "        elif len(available) == 0:\n",
    "            break\n",
    "\n",
    "        num_actions += 1\n",
    "    \n",
    "    if win(state.x):\n",
    "        reward = 1\n",
    "    elif win(state.o):\n",
    "        reward = -1\n",
    "    elif len(available) == 0:\n",
    "        reward = 0\n",
    "    else:\n",
    "        #game not finished: exploitation of the subtree\n",
    "        reward_list = full_game(state, available) #list of all rewards, one for each terminal state\n",
    "        reward = sum(reward_list)/len(reward_list) #let's compute the average result\n",
    "    return trajectory, reward\n",
    "\n",
    "for step in range(100_000):\n",
    "    trajectory, final_reward = random_game()\n",
    "    \n",
    "    for (state, action) in trajectory:\n",
    "        hash_state = (frozenset(state.x), frozenset(state.o), action)\n",
    "        difference = (final_reward - Q_func[hash_state])\n",
    "        Q_func[hash_state] = Q_func[hash_state] + epsilon*difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(Q_dict, state, available):\n",
    "    current_reward = -1000\n",
    "    action = -1\n",
    "    for i in available:\n",
    "        hash_state = (frozenset(state.x), frozenset(state.o), i)\n",
    "        reward = Q_dict[hash_state]\n",
    "        if reward > current_reward:\n",
    "            current_reward = reward\n",
    "            action = i\n",
    "    return action\n",
    "\n",
    "wins = 0\n",
    "draws = 0\n",
    "looses = 0\n",
    "\n",
    "for _ in range(100_000):\n",
    "    state = Position(set(), set())\n",
    "    available = set(range(1, 10))\n",
    "\n",
    "    while True:\n",
    "        #first player\n",
    "        x = agent(Q_func, state, available)\n",
    "        state.x.add(x)\n",
    "        available.remove(x)\n",
    "        \n",
    "        if win(state.x):\n",
    "            wins += 1\n",
    "            break\n",
    "        elif len(available) == 0:\n",
    "            draws += 1\n",
    "            break\n",
    "        \n",
    "        #opponent player (doesn't consider the state)\n",
    "        y = choice(list(available))\n",
    "\n",
    "        state.o.add(y)\n",
    "        available.remove(y)\n",
    "\n",
    "        if win(state.o):\n",
    "            looses += 1\n",
    "            break\n",
    "        elif len(available) == 0:\n",
    "            draws += 1\n",
    "            break\n",
    "print(f'Wins {wins/(wins + looses + draws)}')\n",
    "print(f'Draws {draws/(wins + looses + draws)}')\n",
    "print(f'Looses {looses/(wins + looses + draws)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments and Results\n",
    "\n",
    "We have tried for each agent to change the number the iterations and then tried to play 100_000 games with a random player. Here the results (for each agent we present the number of wins and draws in percentage):\n",
    "\n",
    "| Iterations | Agent1      | Agent2      |\n",
    "|------------|-------------|-------------|\n",
    "| 3_000      | (81.5, 7.5) | (82, 6)     |\n",
    "| 4_000      | (83, 8.5)   | (85, 7)     |\n",
    "| 5_000      | (86, 6)     | (88, 8.5)   |\n",
    "| 10_000     | (90, 5.4)   | (90, 7)     |\n",
    "| 20_000     | (95.3, 3.3) | (87, 8)     |\n",
    "| 50_000     | (97, 2.5)   | (90, 8)     |\n",
    "| 100_000    | (98.7, 1.3) | (93.4, 5.5) |\n",
    "| 200_000    | (99, 1)     | (95, 4)     |\n",
    "| 500_000    | (99, 1)     | (95.6, 4)   |\n",
    "\n",
    "We have the following conclusions:\n",
    "* Thanks to a better exploration of the tree the second agent can achive good results with a lower number of generations.\n",
    "* The first agent can converge quickly to almost perfect performance (with 100_000 iterations) while the second agent is struggling. This may be due to how we treated nodes at lower level, or maybe the epsilon value is too low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
