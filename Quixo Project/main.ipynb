{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work\n",
    "This project was designed, programmed and tested by\n",
    "* Lorenzo Bonannella \n",
    "* Giacomo Fantino\n",
    "* Farisan Fekri\n",
    "* Giacomo Cauda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from game import Game, Move, Player\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "class RandomPlayer(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "        #game.print()\n",
    "        from_pos = (random.randint(0, 4), random.randint(0, 4))\n",
    "        move = random.choice([Move.TOP, Move.BOTTOM, Move.LEFT, Move.RIGHT])\n",
    "        return from_pos, move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMax algorithm\n",
    "\n",
    "We started by using a MinMax strategy.\n",
    "Since the solution tree is huge we have to set a limit to the depth (cut-off). By doing some test we found that the evaluation of non terminal node could use the difference between zeroes and ones: in this way we are forcing our agent to choose to place more zeroes by picking neutral elements than picking zeroes tiles at the beginning of the games. The idea is that by putting more zeroes the chance of winning later is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MinMaxPlayer(Player):\n",
    "    def __init__(self, max_depth=2) -> None:\n",
    "        super().__init__()\n",
    "        self.MAXIMUM_DEPTH = max_depth\n",
    "\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "        #game.print()\n",
    "        _, combination = self.minmax(game, True, 0)\n",
    "        return combination\n",
    "    \n",
    "    def minmax(self, game, maximizing_player, depth):\n",
    "        # The MinMax algorithm function\n",
    "        # It returns an evalution which a score for a certain player and the best move it can do\n",
    "        if (game.check_winner() != -1 or depth==self.MAXIMUM_DEPTH):\n",
    "            if game.check_winner() == 0:\n",
    "                return 1, None\n",
    "            elif game.check_winner() == 1:\n",
    "                return -1, None\n",
    "            else: # depth==self.MAXIMUM_DEPTH:\n",
    "                #more zeroes the better since we are player 0\n",
    "                num_zeros = np.count_nonzero(game._board == 0)\n",
    "                num_ones = np.count_nonzero(game._board == 1)\n",
    "                return (num_zeros - num_ones)/100, None\n",
    "            #we take the solution with the highest number of 0\n",
    "\n",
    "        depth = depth + 1 #increase depth        \n",
    "        if maximizing_player:\n",
    "            max_eval = float('-inf')\n",
    "            best_move = None\n",
    "            for possible_move in self.get_possible_moves(game, 0): #first player \n",
    "                new_game = self.get_new_board(game, possible_move, 0)\n",
    "                eval, _ = self.minmax(new_game, False, depth)\n",
    "                if eval > max_eval:\n",
    "                    max_eval = eval\n",
    "                    best_move = possible_move\n",
    "            return max_eval, best_move\n",
    "        else:\n",
    "            min_eval = float('inf')\n",
    "            best_move = None\n",
    "            for possible_move in self.get_possible_moves(game, 1):  #second player \n",
    "                new_game = self.get_new_board(game, possible_move, 1)\n",
    "                eval, _ = self.minmax(new_game, True, depth)\n",
    "                if eval < min_eval:\n",
    "                    min_eval = eval\n",
    "                    best_move = possible_move\n",
    "            return min_eval, best_move\n",
    "    \n",
    "    def get_new_board(self, game, possible_move, player):\n",
    "        #We do a deepcopy of the game object because in MinMax we change the game not only the board attribute\n",
    "        new_game = deepcopy(game)\n",
    "        pos, move = possible_move\n",
    "        res = new_game.move(pos, move, player) #we are trying the move \n",
    "        if not res:\n",
    "            print(f'ERROR: SHOULDNT RECEIVE FALSE on {pos} and {move} player {player}')\n",
    "            game.print()\n",
    "            sys.exit(1)\n",
    "        return new_game\n",
    "\n",
    "    def acceptable_move(self, pos, mov):\n",
    "         return not ((pos[1] == 0 and mov == Move.TOP) \n",
    "                     or (pos[1] == 4 and mov == Move.BOTTOM) \n",
    "                     or (pos[0] == 0 and mov == Move.LEFT)\n",
    "                     or (pos[0] == 4 and mov == Move.RIGHT))\n",
    "\n",
    "    def get_possible_moves(self, game, player):\n",
    "        possible_pos = [(i, j) for i in range(0,5) for j in range(0, 5) \n",
    "                            if (i == 0 or i == 4 or j == 0 or j == 4) \n",
    "                            and (game._board[j,i] == -1 or game._board[j,i] == player)]\n",
    "        \n",
    "        possible_moves_pos = [(pos, mov) for pos in possible_pos for mov in [Move.TOP, Move.BOTTOM, Move.LEFT, Move.RIGHT]]\n",
    "        #check if position and moves are feasable\n",
    "        possible_moves_pos = [(pos, mov) for pos, mov in possible_moves_pos if self.acceptable_move(pos, mov)]\n",
    "        return possible_moves_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide the limit of the depth we measured the time to evaluate the initial state (being that is the one with higher number of possible moves) for each possible depth. By looking at the results we decided to use depth equal to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for depth 1: 0.001154184341430664 seconds\n",
      "Execution time for depth 2: 0.5223915576934814 seconds\n",
      "Execution time for depth 3: 18.050224542617798 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "initial_game = Game() #the empty board is the one it takes longer time\n",
    "\n",
    "for depth in range(1, 4):\n",
    "    player = MinMaxPlayer(max_depth=depth)\n",
    "    start_time = time.time()\n",
    "\n",
    "    _ = player.make_move(initial_game)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Execution time for depth {depth}: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the function for testing our algorithm against a random player. We can see that our strategy was correct since we got almost 100% of winning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins = 0\n",
    "player1 = MinMaxPlayer()\n",
    "player2 = RandomPlayer()\n",
    "for i in range(1000):\n",
    "    print(f'starting game {i}')\n",
    "    g = Game()\n",
    "    winner = g.play(player1, player2)\n",
    "    \n",
    "    if winner == 0:\n",
    "        wins += 1\n",
    "\n",
    "print(f'I won {wins} time out of 1000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMax algorithm with Alpha-Beta Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the depth is very low due to the branching factor of the algorithm we decided to use alpha beta pruning to avoid to compute most of the subtrees.\n",
    "The strategy is the same as before (picking more zeroes than one) but now the _minmax function includes the alpha beta pruning.\n",
    "The basic idea is: in the case of the max player we receive from the parent node (min player) in the the current minimum value, in the variable beta. If we find a value which is higher than the minimum we can immediately stop because we won't change the decision of the parent node. The same happens when we are computing the min player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxPlayer(Player):\n",
    "    def __init__(self, max_depth=4) -> None:\n",
    "        super().__init__()\n",
    "        self.MAXIMUM_DEPTH = max_depth\n",
    "\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "        #game.print()\n",
    "        _, combination = self.minmax_alpha_beta(game, 0, -float('inf'), float('inf'), True)\n",
    "        return combination\n",
    "\n",
    "    def minmax_alpha_beta(self, game, depth, alpha, beta, maximizing_player):\n",
    "        # The MinMax algorithm function with Alpha-Beta Pruning\n",
    "        # alpha = current maximum value that we have found on that sub-branch\n",
    "        # beta  = current minimum value that we have on that sub-branch\n",
    "        # Base case: If the game is over or maximum depth is reached\n",
    "        if (game.check_winner() != -1 or depth==self.MAXIMUM_DEPTH):\n",
    "            if game.check_winner() == 0:\n",
    "                return 1, None\n",
    "            elif game.check_winner() == 1:\n",
    "                return -1, None\n",
    "            else: # depth==self.MAXIMUM_DEPTH \n",
    "                #more zeroes the better since we are player 0\n",
    "                num_zeros = np.count_nonzero(game._board == 0)\n",
    "                num_ones = np.count_nonzero(game._board == 1)\n",
    "                return (num_zeros - num_ones)/100, None \n",
    "            #we take the solution with the highest number of 0\n",
    "            \n",
    "        depth = depth + 1 #increase depth \n",
    "        \n",
    "        if maximizing_player:\n",
    "            max_eval = float('-inf')\n",
    "            best_move = None\n",
    "            for possible_move in self.get_possible_moves(game, 0):\n",
    "                new_game = self.get_new_board(game, possible_move, 0)\n",
    "                eval, _ = self.minmax_alpha_beta(new_game, depth, alpha, beta, False)\n",
    "                if eval > max_eval:\n",
    "                    max_eval = eval\n",
    "                    best_move = possible_move\n",
    "                alpha = max(alpha, eval)\n",
    "                if beta <= alpha:\n",
    "                    break  # Beta cutoff    \n",
    "            return max_eval, best_move\n",
    "        else:\n",
    "            min_eval = float('inf')\n",
    "            best_move = None\n",
    "            for possible_move in self.get_possible_moves(game, 1):  #second player\n",
    "                new_game = self.get_new_board(game, possible_move, 1)\n",
    "                eval, _ = self.minmax_alpha_beta(new_game, depth, alpha, beta, True)\n",
    "                if eval < min_eval:\n",
    "                    min_eval = eval\n",
    "                    best_move = possible_move\n",
    "                beta = min(beta, eval)\n",
    "                if alpha > beta:\n",
    "                    break  # Alpha cutoff\n",
    "            return min_eval, best_move\n",
    "    \n",
    "    def get_new_board(self, game, possible_move, player):\n",
    "        #We do a deepcopy of the game object because in MinMax we change the game not only the board attribute\n",
    "        new_game = deepcopy(game)\n",
    "        pos, move = possible_move\n",
    "        res = new_game.move(pos, move, player) #we are trying the move \n",
    "        if not res:\n",
    "            print(f'ERROR: SHOULDNT RECEIVE FALSE on {pos} and {move} player {player}')\n",
    "            game.print()\n",
    "            sys.exit(1)\n",
    "        return new_game\n",
    "\n",
    "    def acceptable_move(self, pos, mov):\n",
    "         return not ((pos[1] == 0 and mov == Move.TOP) \n",
    "                     or (pos[1] == 4 and mov == Move.BOTTOM) \n",
    "                     or (pos[0] == 0 and mov == Move.LEFT) \n",
    "                     or (pos[0] == 4 and mov == Move.RIGHT))\n",
    "\n",
    "    def get_possible_moves(self, game, player):\n",
    "        possible_pos = [(i, j) for i in range(0,5) for j in range(0, 5) \n",
    "                            if (i == 0 or i == 4 or j == 0 or j == 4) \n",
    "                            and (game._board[j,i] == -1 or game._board[j,i] == player)]\n",
    "        \n",
    "        possible_moves_pos = [(pos, mov) for pos in possible_pos for mov in [Move.TOP, Move.BOTTOM, Move.LEFT, Move.RIGHT]]\n",
    "        #check if position and moves are feasable\n",
    "        possible_moves_pos = [(pos, mov) for pos, mov in possible_moves_pos if self.acceptable_move(pos, mov)]\n",
    "        return possible_moves_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By reusing the same function as before we can see the huge improvement of using alpha beta pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for depth 1: 0.014258146286010742 seconds\n",
      "Execution time for depth 2: 0.03733420372009277 seconds\n",
      "Execution time for depth 3: 0.4546060562133789 seconds\n",
      "Execution time for depth 4: 1.267000436782837 seconds\n",
      "Execution time for depth 5: 21.52586340904236 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "initial_game = Game() #the empty board is the one it takes longer time\n",
    "\n",
    "for depth in range(1, 6):\n",
    "    player = MinMaxPlayer(max_depth=depth)\n",
    "    start_time = time.time()\n",
    "\n",
    "    _ = player.make_move(initial_game)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Execution time for depth {depth}: {end_time - start_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are the same as before but by printing the board after each move we concluded that the chosen moves done by minmax were more rational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins = 0\n",
    "player1 = MinMaxPlayer()\n",
    "player2 = RandomPlayer()\n",
    "for i in range(1000):\n",
    "    print(f'starting game {i}')\n",
    "    g = Game()\n",
    "    winner = g.play(player1, player2)\n",
    "    \n",
    "    if winner == 0:\n",
    "        wins += 1\n",
    "\n",
    "print(f'I won {wins} time out of 1000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning with Monte Carlo approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to test a Reinforcement learning with this game: the agent will plays a certain number of games in a training phase to estimate a Q_function. We assume that by playing a lot of games the Q value of a node will converge to the expected reward of that node. Still we had some doubt since the tree is huge, so it may takes a lot of iterations to actually explore it.\n",
    "\n",
    "The Monte Carlo approach is the building base of this algorithm: we play a set of random games, see the results and use those to update all the nodes in the trajectories. We used a learning rate (the update weight) of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "from collections import defaultdict\n",
    "\n",
    "class RL_Player(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.Q_func = defaultdict(float)\n",
    "\n",
    "    '''\n",
    "    Once we trained the agent, we use this function to play\n",
    "    '''\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "        #game.print()\n",
    "        moves = self.get_possible_moves(game, 0)\n",
    "        #we assume we want the maximum reward possible\n",
    "        reward = -float('inf')\n",
    "        best_comb = None\n",
    "        board_tuple = tuple(map(tuple, game._board))\n",
    "        #let's get the best action for this scenario from the Q-function\n",
    "        for mov in moves:\n",
    "            rew = self.Q_func[(board_tuple, mov)]\n",
    "            if rew > reward:\n",
    "                reward = rew\n",
    "                best_comb = mov\n",
    "        \n",
    "        from_pos, move = best_comb\n",
    "        return from_pos, move\n",
    "    \n",
    "    def __random_game(self):\n",
    "        state = Game()\n",
    "        trajectory = []\n",
    "\n",
    "        while True:\n",
    "            #first player is US\n",
    "            x = choice(self.get_possible_moves(state, 0))\n",
    "            trajectory.append((deepcopy(state), x)) #current state + chosen action\n",
    "            pos, move = x\n",
    "            state.move(pos, move, 0)\n",
    "\n",
    "            #check_winner != -1 because i can make a bad action\n",
    "            if state.check_winner() != -1:\n",
    "                break\n",
    "\n",
    "            #opponent player (doesn't consider the state)\n",
    "            y = choice(self.get_possible_moves(state, 1))\n",
    "            pos, move = y\n",
    "            state.move(pos, move, 1)\n",
    "\n",
    "            if state.check_winner() != -1:\n",
    "                break\n",
    "            \n",
    "        return trajectory, state #sequence of move for computing the state value function\n",
    "\n",
    "    def state_value(self, state):\n",
    "        return 1 if state.check_winner() == 0 else -1\n",
    "    \n",
    "    def train_agent(self, iterations, epsilon = 0.01):\n",
    "        for _ in range(iterations):\n",
    "            #print(f\"ITERATION {i}\")\n",
    "            trajectory, last_state = self.__random_game()\n",
    "            final_reward = self.state_value(last_state)\n",
    "            for (state, action) in trajectory:\n",
    "                #convert numpy to tuple to make it hashable\n",
    "                board_tuple = tuple(map(tuple, state._board))\n",
    "                hash_state = (board_tuple, action)\n",
    "                difference = (final_reward - self.Q_func[hash_state])\n",
    "                self.Q_func[hash_state] = self.Q_func[hash_state] + epsilon*difference\n",
    "    \n",
    "\n",
    "    def get_possible_moves(self, game, player):\n",
    "        possible_pos = [(i, j) for i in range(0,5) for j in range(0, 5) \n",
    "                            if (i == 0 or i == 4 or j == 0 or j == 4) \n",
    "                            and (game._board[j,i] == -1 or game._board[j,i] == player)]\n",
    "        \n",
    "        possible_moves_pos = [(pos, mov) for pos in possible_pos for mov in [Move.TOP, Move.BOTTOM, Move.LEFT, Move.RIGHT]]\n",
    "        #check if position and moves are feasable\n",
    "        possible_moves_pos = [(pos, mov) for pos, mov in possible_moves_pos if self.acceptable_move(pos, mov)]\n",
    "        return possible_moves_pos\n",
    "\n",
    "    def acceptable_move(self, pos, mov):\n",
    "         return not ((pos[1] == 0 and mov == Move.TOP) \n",
    "                     or (pos[1] == 4 and mov == Move.BOTTOM) \n",
    "                     or (pos[0] == 0 and mov == Move.LEFT)\n",
    "                     or (pos[0] == 4 and mov == Move.RIGHT))\n",
    "\n",
    "    def best_action_states(self, n):\n",
    "        top = (sorted(self.Q_func.items(), key=lambda e : e[1], reverse = True)[:n])\n",
    "        for key, value in top:\n",
    "            print(f'key : {key}')\n",
    "            print(f'value: {value}')\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the code to train and then test our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player1 = RL_Player()\n",
    "player2 = RandomPlayer()\n",
    "\n",
    "training_its = 200_000\n",
    "player1.train_agent(training_its)\n",
    "\n",
    "win = 0\n",
    "for _ in range(1000):\n",
    "    g = Game()\n",
    "    winner = g.play(player1, player2)\n",
    "    if winner == 0:\n",
    "        win += 1\n",
    "\n",
    "print(f'We won {win} out of 1000 games')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our doubts were correct: with 200_000 iterations the process took more than one hour and the results were not interesting. We tried changing the iterations and the learning rate:\n",
    "\n",
    "<img src=\"./loss.jpeg\" alt=\"Plot of the loss functions\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning with Monte Carlo approach and Parallization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this last agent we added two strategies to the RL:\n",
    "* now it's possible to save the Q function and reload it later to be trained. This gave us the possibilites to train our agent for many iterations without our computers crashing.\n",
    "* Since the computation of a single game is independent from the others we used Thread Parallelization to speed up the execution.\n",
    "\n",
    "As we can see from the code we used two optimizations: ThreadPool and Future functions.\n",
    "1) With threadpool we don't create a thread for each function call but only a fixed amount. Each thread will look at the queue and execute the function.\n",
    "2) With Future we also parallelize the processing of the results: instead of waiting for all the threads to finish each time a thread stop we immediately process the result in a callback-fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import pickle\n",
    "\n",
    "class RL_Player(Player):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.Q_func = defaultdict(float)\n",
    "\n",
    "    '''\n",
    "    Once we trained the agent, we use this function to play\n",
    "    '''\n",
    "    def make_move(self, game: 'Game') -> tuple[tuple[int, int], Move]:\n",
    "        #game.print()\n",
    "        moves = self.get_possible_moves(game, 0)\n",
    "        #we assume we want the maximum reward possible\n",
    "        reward = -float('inf')\n",
    "        best_comb = None\n",
    "        board_tuple = tuple(map(tuple, game._board))\n",
    "        #let's get the best action for this scenario from the Q-function\n",
    "        for mov in moves:\n",
    "            rew = self.Q_func[(board_tuple, mov)]\n",
    "            if rew > reward:\n",
    "                reward = rew\n",
    "                best_comb = mov\n",
    "        \n",
    "        from_pos, move = best_comb\n",
    "        return from_pos, move\n",
    "    \n",
    "    def _random_game(self):\n",
    "        state = Game()\n",
    "        trajectory = []\n",
    "\n",
    "        while True:\n",
    "            #first player is US\n",
    "            x = choice(self.get_possible_moves(state, 0))\n",
    "            trajectory.append((deepcopy(state), x)) #current state + chosen action\n",
    "            pos, move = x\n",
    "            state.move(pos, move, 0)\n",
    "\n",
    "            #check_winner != -1 because i can make a bad action\n",
    "            if state.check_winner() != -1:\n",
    "                break\n",
    "\n",
    "            #opponent player (doesn't consider the state)\n",
    "            y = choice(self.get_possible_moves(state, 1))\n",
    "            pos, move = y\n",
    "            state.move(pos, move, 1)\n",
    "\n",
    "            if state.check_winner() != -1:\n",
    "                break\n",
    "            \n",
    "        return trajectory, state #sequence of move for computing the state value function\n",
    "\n",
    "    def state_value(self, state):\n",
    "        return 1 if state.check_winner() == 0 else -1\n",
    "    \n",
    "    def update_Q_func(self, hash_state, final_reward, epsilon):\n",
    "        difference = (final_reward - self.Q_func[hash_state])\n",
    "        self.Q_func[hash_state] = self.Q_func[hash_state] + epsilon * difference\n",
    "    \n",
    "    def train_agent(self, iterations, epsilon=0.01, save_dictionary=False):\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = [executor.submit(self._random_game) for _ in range(iterations)]\n",
    "\n",
    "            i = 1\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                i+=1\n",
    "                print(f'process {i} finished')\n",
    "\n",
    "                try:\n",
    "                    trajectory, last_state = future.result()\n",
    "                except Exception as e:\n",
    "                    print(f'Error in process {i}: {e}')\n",
    "                    continue  # Continue to the next iteration if there was an exception\n",
    "\n",
    "                final_reward = self.state_value(last_state)\n",
    "\n",
    "                for (state, action) in trajectory:\n",
    "                    board_tuple = tuple(map(tuple, state._board))\n",
    "                    hash_state = (board_tuple, action)\n",
    "                    self.update_Q_func(hash_state, final_reward, epsilon)\n",
    "    \n",
    "        if save_dictionary:          \n",
    "            # Specify the file path where you want to save the dictionary\n",
    "            file_path = 'my_dictionary.pickle'\n",
    "\n",
    "            pickle.dump(self.Q_func, open(file_path, 'wb')) #in case erase the old file\n",
    "    \n",
    "    def load_from_dictionary(self, file_path = 'my_dictionary.pickle'):\n",
    "        # Load the dictionary from the file\n",
    "        self.Q_func = pickle.load(open(file_path, 'rb'))\n",
    "    \n",
    "    def get_possible_moves(self, game, player):\n",
    "        possible_pos = [(i, j) for i in range(0,5) for j in range(0, 5) \n",
    "                            if (i == 0 or i == 4 or j == 0 or j == 4) \n",
    "                            and (game._board[j,i] == -1 or game._board[j,i] == player)]\n",
    "        \n",
    "        possible_moves_pos = [(pos, mov) for pos in possible_pos for mov in [Move.TOP, Move.BOTTOM, Move.LEFT, Move.RIGHT]]\n",
    "        #check if position and moves are feasable\n",
    "        possible_moves_pos = [(pos, mov) for pos, mov in possible_moves_pos if self.acceptable_move(pos, mov)]\n",
    "        return possible_moves_pos\n",
    "\n",
    "    def acceptable_move(self, pos, mov):\n",
    "         return not ((pos[1] == 0 and mov == Move.TOP) \n",
    "                     or (pos[1] == 4 and mov == Move.BOTTOM) \n",
    "                     or (pos[0] == 0 and mov == Move.LEFT)\n",
    "                     or (pos[0] == 4 and mov == Move.RIGHT))\n",
    "\n",
    "    def best_action_states(self, n):\n",
    "        top = (sorted(self.Q_func.items(), key=lambda e : e[1], reverse = True)[:n])\n",
    "        for key, value in top:\n",
    "            print(f'key : {key}')\n",
    "            print(f'value: {value}')\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start by seeing the improvements in the time:\n",
    "\n",
    "| Iterations | old RL       | improvements|\n",
    "|------------|--------------|-------------|\n",
    "| 50_000     | 3 min 40 sec | -30 sec     |\n",
    "| 200_000    | 1 hour 5 min | -20 min     |\n",
    "\n",
    "We can see a correlations between iterations and saved time.\n",
    "\n",
    "Performances wise we were able to reach more of a milion iterations by retraining the agent more times. Even by exploring a lot (the pickle file became 5GB) still the exploration was not enough and the results were only roughly 33% of winnings. This confirm the problem of exploring the entire quixo state space for the RL agent and confirm the minmax as the best agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training phase\n",
    "player1 = RL_Player()\n",
    "player2 = RandomPlayer()\n",
    "\n",
    "iterations = 200_000\n",
    "player1.load_from_dictionary()\n",
    "player1.train_agent(iterations, save_dictionary=True)\n",
    "#save_dictionary: if it's true it overrides the file with the dictionary with the new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test phase\n",
    "player1 = RL_Player()\n",
    "player2 = RandomPlayer()\n",
    "\n",
    "player1.load_from_dictionary()\n",
    "\n",
    "win = 0\n",
    "for _ in range(1000):\n",
    "    g = Game()\n",
    "    winner = g.play(player1, player2)\n",
    "    if winner == 0:\n",
    "        win += 1\n",
    "\n",
    "print(f'We won {win} out of 1000 games')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
